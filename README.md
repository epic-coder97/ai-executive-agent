
# ai-executive-agent-portfolio

An end-to-end demo of an **Executive Assistant AI Agent** that can plan tasks, use tools, keep memory, run evals, apply guardrails with HITL, choose between prompting vs RAG vs fine-tuning, and integrate with productivity tools through zero-cost mocks.  
Built to showcase portfolio readiness **without paying for APIs**.

---

## ğŸš€ What This Demo Proves

- **Agents that plan and use tools**  
  Decomposes long-horizon tasks, calls tools (Calendar, Email, Slack, Expenses), and persists state in SQLite.

- **Evals with feedback loops**  
  Scripted scenarios measure task success, log failure modes, and produce metrics.

- **Guardrails + Human-in-the-Loop (HITL)**  
  Risky actions (like sending email or creating calendar events) require confirmation. Unsafe actions are blocked.

- **Prompting vs RAG vs Fine-Tune**  
  Side-by-side demos show when prompting is enough, when RAG is required, and why fine-tuning is unnecessary here.

- **Integrations**  
  Google Calendar, Gmail, Slack, Expensify, Zoom are **mocked adapters** that mirror real APIs.

- **Human workflow mapping**  
  Identifies where EAs spend time and automates 1â€“2 tasks.

---

## ğŸ›  Tech Stack

- Python 3.11  
- Streamlit (UI)  
- SQLite (agent memory + logs)  
- FAISS or Chroma (RAG index)  
- Pydantic (schemas)  
- PyTest (testing)  

---

## ğŸ“‚ Project Structure

```

src/
app.py                  # Streamlit UI
agent/
planner.py            # task decomposition + execution
tools.py              # tool registry + adapters
memory.py             # SQLite-based memory
guardrails.py         # rules + HITL confirmation
rag.py                # RAG over /kb documents
mocks/
gmail.py              # mock Gmail
gcal.py               # mock Google Calendar
slack.py              # mock Slack
expensify.py          # mock Expenses
zoom.py               # mock Zoom
evals/
scenarios.yaml        # scripted tasks
run\_evals.py          # evaluation harness
kb/
handbook.pdf          # example source for RAG
data/
runs.sqlite           # created at runtime
tests/
test\_agent\_happy.py
test\_guardrails.py

````

---

## âš¡ Quickstart

### 1. Setup environment
```bash
python -m venv .venv
source .venv/bin/activate    # Windows: .venv\Scripts\activate
pip install --upgrade pip
pip install -r requirements.txt
````

### 2. Add a document

Place a sample PDF (e.g., fake handbook) into `kb/`.

### 3. Run the app

```bash
streamlit run src/app.py
```

### 4. Run evals

```bash
python src/evals/run_evals.py
```

---

## ğŸ¬ Demo Flow

1. **Plan the week**

   > â€œPlan my next 7 days with daily standup at 9 am and DS interview prep.â€
   > Restart app â†’ ask *â€œWhat did we plan yesterday?â€* â†’ proves memory.

2. **Scheduling with HITL**

   > â€œFind a 30-min slot with Alex next Tuesday. Draft email but donâ€™t send until I approve.â€
   > Guardrail waits for **Approve** click.

3. **RAG vs Prompting**

   > â€œWhat is our expense approval policy?â€
   > Show ungrounded vs grounded answers.

4. **Slack + Zoom**

   > â€œPost a status update in #ops and create a Zoom link.â€
   > Goes through mock adapters, logged in SQLite.

---

## ğŸ›¡ Guardrails + HITL

* Unsafe phrases blocked automatically.
* Email + Calendar require confirmation.
* Slack posts scrubbed for PII before â€œsendingâ€.

---

## ğŸ“Š Evals

* `evals/scenarios.yaml` â†’ defines scripted tasks & acceptance checks.
* `evals/run_evals.py` â†’ runs all, reports:

  * âœ… Success rate
  * ğŸ”§ Tool-call precision
  * ğŸ•‘ Avg steps per task
  * âŒ Failure tags (hallucination, drift, misuse, timeout)

---

## ğŸ’¾ Memory Model

* **Short-term:** context for current session.
* **Long-term:** stored in SQLite, recalled by topic.
* Reloaded at startup to maintain continuity.

---

## ğŸ“š RAG Implementation

* PDFs in `kb/` â†’ chunked + embedded.
* Retrieval with top-k + rerank.
* Citations displayed in UI.

---

## ğŸ“Œ Requirements

`requirements.txt`

```
streamlit
pydantic
faiss-cpu
chromadb
pypdf
tiktoken
sqlalchemy
python-dotenv
pytest
```

---

## ğŸ—º Roadmap

* Add calendar conflict resolver with multiple options
* Mock expense OCR with sample receipts
* Richer evals with noisy inputs
* Exportable run reports (CSV)

---

## ğŸ“„ License

MIT

